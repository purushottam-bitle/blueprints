Hereâ€™s a clear and structured **CLM summary with blocker details**, formatted to be used in documentation, presentations, or status updates:

---

### âœ… **CLM Automation Test Execution Flow**

#### ğŸ”¹ **Overview**

As part of the **CLM (Continuous Lifecycle Management)** platform, the system enables scheduling and executing Robot Framework test runs across distributed test beds (nodes) and collecting test results centrally.

---

#### ğŸ”¸ **Workflow Summary**

1. **Test Run Scheduling via CLM UI:**

   * User selects test suite(s) and triggers the test run.
   * A **test run entry** is saved in the database with status `queued`.

2. **Background Queue Processor:**

   * A Python background task continuously polls for `queued` test runs.
   * When required devices are available on a test bed:

     * It allocates devices.
     * Sends an API call to the **Device Farming microservice** to start the task.

3. **Device Farming Microservice (DFMS):**

   * Accepts the test task.
   * Generates appropriate Robot or Pabot commands.
   * Executes the test run via `subprocess` on the test bed.
   * Saves logs, output, and task status in DB.

4. **Core Microservice (CLM Core):**

   * Tracks task execution status (`running`, `completed`, `failed`, etc.).
   * Supports retry, cancel, and real-time monitoring via APIs.
   * Aggregates results (via DAG or external utility like `rebot`) into a central report.

---

### ğŸ”´ **Blocker Issue**

#### âŒ **Problem Statement:**

When running **multiple Robot Framework commands via subprocess**, only **one command completes successfully**, while others **fail**, even with:

* Separate `.robot` files
* Different test case IDs
* Isolated users or sessions per command
* Separate machines per process

---

### ğŸ§ª **Solutions Attempted (and Failed):**

| Attempt                   | Description                                                     | Result                                  |
| ------------------------- | --------------------------------------------------------------- | --------------------------------------- |
| âœ… Sequential runs         | Run commands one after the other on same or different test beds | âŒ Failed                                |
| âœ… Unique users            | Use different system users per test run                         | âŒ Failed                                |
| âœ… Machine isolation       | Run each test case on a separate VM/test bed                    | âŒ Still faced conflicts or errors       |
| âœ… Unique Robot output dir | Ensured different `--outputdir` per run                         | âŒ Did not help                          |
| âœ… Separate task\_id/logs  | All logs were unique per task                                   | âŒ Logs still showed one task dominating |

---

### ğŸ” **Possible Root Causes to Investigate**

1. **Shared resource collisions** (e.g., environment vars, port usage, hardware like ADB/USB conflicts).
2. **Robot / subprocess shell not fully isolated** when launched via `subprocess.Popen`.
3. **Device contention or locking failures**, where one test consumes all input/output channels.
4. **Reentrant code or shared config (e.g., environment variables or `resource.robot`)** being overwritten.

---

### âœ… **Next Steps & Suggestions**

* âœ”ï¸ **Use `pabot`** for proper parallelism if multiple suites share the same environment.
* âœ”ï¸ **Run subprocesses with fully isolated working directories and environments**.
* âœ”ï¸ **Containerize each test run** (e.g., Docker per test bed) to avoid host-level conflicts.
* âœ”ï¸ Validate with `psutil` or similar to track overlapping PIDs or resource access.
* âœ”ï¸ Ensure **no device (`DUT1`) is reused** across concurrent test runs.
* â“ Consider switching from `subprocess` to **async process pools** or **Celery workers** with explicit queueing.

---

Would you like a diagnostic script or Docker-based isolation approach to help debug the conflict?



Subject: CLM Test Execution Summary and Subprocess Execution Blocker

Hi Team,

As part of the CLM (Continuous Lifecycle Management) integration, here is a summary of the current implementation and a key blocker encountered during multi-suite test execution.

âœ… CLM Automation Execution Summary
Overview:
The CLM platform supports automated test execution using Robot Framework across distributed test beds. Tests can be scheduled via the CLM UI, and results are collected centrally after execution.

Execution Flow:

Test Scheduling:
Users trigger test runs via the CLM UI. Each request is stored in the database with a queued status.

Queue Processor:
A background Python service monitors for queued test runs, checks device availability, and assigns suitable test beds.

Device Farming Microservice (DFMS):

Creates the test task

Constructs Robot Framework commands

Executes tests via subprocess

Tracks logs and updates status

Core Microservice:

Monitors running tasks

Updates task status (running, completed, etc.)

Supports task tracking and reporting

ğŸ”´ Blocker Issue
Problem:
When triggering multiple Robot Framework executions via subprocess, only one test run completes successfully, while all others fail â€” even when run with isolation mechanisms in place.

Troubleshooting Attempts:

#	Attempt	Outcome
1	Sequential execution with unique test cases	âŒ Failed
2	Unique system users per process	âŒ Failed
3	Execution on separate machines (test beds)	âŒ Failed
4	Unique --outputdir and log files	âŒ Failed
5	Dedicated sessions per process	âŒ Failed
6	Separate logs, commands, and task records	âŒ Failed; only one test consistently succeeded

We are continuing to investigate this blocker to identify the root cause and enable reliable parallel execution.

Please let me know if you need any further details.
